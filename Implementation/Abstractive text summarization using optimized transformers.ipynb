{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "699454f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries installations\n",
    "# !pip install transformers -q\n",
    "# !pip install torch\n",
    "# !pip install datsets transformers[sentencepiece]\n",
    "# !pip install sentencepiece\n",
    "# !pip install transformers==2.9.0 \n",
    "# !pip install pytorch_lightning==0.7.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e6a54e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing stock libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03334091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a custom dataset for reading the dataframe and loading it into the dataloader to pass it to the neural network \n",
    "# at a later stage for finetuning the model and to prepare it for predictions\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = summ_len\n",
    "        self.text = self.data.text\n",
    "        self.ctext = self.data.ctext\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ctext = str(self.ctext[index])\n",
    "        ctext = ' '.join(ctext.split())\n",
    "\n",
    "        text = str(self.text[index])\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
    "        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
    "\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'source_ids': source_ids.to(dtype=torch.long), \n",
    "            'source_mask': source_mask.to(dtype=torch.long), \n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3da0a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training function. This will be called in the main function. It is run depending on the epoch value.\n",
    "# The model is put into train mode and then we wnumerate over the training loader and passed to the defined network \n",
    "\n",
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "    model.train()\n",
    "    for _,data in enumerate(loader, 0):\n",
    "        y = data['target_ids'].to(device, dtype = torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        if _%500==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # xm.optimizer_step(optimizer)\n",
    "        # xm.mark_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0dad09bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch, tokenizer, model, device, loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            y = data['target_ids'].to(device, dtype = torch.long)\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids = ids,\n",
    "                attention_mask = mask, \n",
    "                max_length=150, \n",
    "                num_beams=2,\n",
    "                repetition_penalty=2.5, \n",
    "                length_penalty=1.0, \n",
    "                early_stopping=True\n",
    "                )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "            if _%100==0:\n",
    "                print(f'Completed {_}')\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "    return predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f6dbcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\n",
    "    VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n",
    "    TRAIN_EPOCHS = 2        # number of epochs to train (default: 10)\n",
    "    VAL_EPOCHS = 1 \n",
    "    LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n",
    "    SEED = 42               # random seed (default: 42)\n",
    "    MAX_LEN = 512\n",
    "    SUMMARY_LEN = 150 \n",
    "\n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    torch.manual_seed(SEED) # pytorch random seed\n",
    "    np.random.seed(SEED) # numpy random seed\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # tokenzier for encoding the text\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "    \n",
    "\n",
    "    # Importing and Pre-Processing the domain data\n",
    "    # Selecting the needed columns only. \n",
    "    # Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task. \n",
    "    df = pd.read_csv('./dataset/news_summary.csv',encoding='latin-1')\n",
    "    df = df[['text','ctext']]\n",
    "    df.ctext = 'summarize: ' + df.ctext\n",
    "    print(df.head())\n",
    "\n",
    "    \n",
    "    # Creation of Dataset and Dataloader\n",
    "    # Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \n",
    "    train_size = 0.8\n",
    "    train_dataset=df.sample(frac=train_size, random_state = SEED).reset_index(drop=True)\n",
    "    val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "\n",
    "    print(\"FULL Dataset: {}\".format(df.shape))\n",
    "    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "    print(\"TEST Dataset: {}\".format(val_dataset.shape))\n",
    "\n",
    "\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
    "    val_set = CustomDataset(val_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
    "\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    train_params = {\n",
    "        'batch_size': TRAIN_BATCH_SIZE,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0\n",
    "        }\n",
    "\n",
    "    val_params = {\n",
    "        'batch_size': VALID_BATCH_SIZE,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 0\n",
    "        }\n",
    "\n",
    "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "\n",
    "    \n",
    "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
    "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
    "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Training loop\n",
    "    print('Initiating Fine-Tuning for the model on our dataset')\n",
    "\n",
    "    for epoch in range(TRAIN_EPOCHS):\n",
    "        train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
    "\n",
    "\n",
    "    # Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n",
    "    # Saving the dataframe as predictions.csv\n",
    "    print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
    "    for epoch in range(VAL_EPOCHS):\n",
    "        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
    "        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "        final_df.to_csv('dataset/predictions.csv')\n",
    "        print('Output Files generated for review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4cd7c5e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6548\\451043146.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6548\\544971751.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# tokenzier for encoding the text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"t5-base\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "617faf7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                      Version\n",
      "---------------------------- -----------\n",
      "absl-py                      1.0.0\n",
      "aniso8601                    9.0.1\n",
      "argon2-cffi                  21.3.0\n",
      "argon2-cffi-bindings         21.2.0\n",
      "astor                        0.8.1\n",
      "astunparse                   1.6.3\n",
      "attrs                        21.4.0\n",
      "autocorrect                  2.6.1\n",
      "backcall                     0.2.0\n",
      "beautifulsoup4               4.11.1\n",
      "bleach                       5.0.0\n",
      "blis                         0.9.1\n",
      "cached-property              1.5.2\n",
      "cachetools                   4.2.4\n",
      "catalogue                    2.0.8\n",
      "certifi                      2021.10.8\n",
      "cffi                         1.15.0\n",
      "charset-normalizer           2.0.12\n",
      "click                        8.1.3\n",
      "colorama                     0.4.4\n",
      "confection                   0.0.1\n",
      "cycler                       0.11.0\n",
      "cymem                        2.0.6\n",
      "debugpy                      1.6.0\n",
      "decorator                    5.1.1\n",
      "defusedxml                   0.7.1\n",
      "distlib                      0.3.6\n",
      "docker-pycreds               0.4.0\n",
      "docopt                       0.6.2\n",
      "editdistance                 0.6.0\n",
      "entrypoints                  0.4\n",
      "fastjsonschema               2.15.3\n",
      "filelock                     3.8.0\n",
      "Flask                        2.1.2\n",
      "Flask-Cors                   3.0.10\n",
      "Flask-RESTful                0.3.9\n",
      "flatbuffers                  1.12\n",
      "fonttools                    4.33.3\n",
      "future                       0.18.2\n",
      "gast                         0.2.2\n",
      "gitdb                        4.0.9\n",
      "GitPython                    3.1.29\n",
      "google-auth                  1.35.0\n",
      "google-auth-oauthlib         0.4.6\n",
      "google-pasta                 0.2.0\n",
      "grpcio                       1.44.0\n",
      "h5py                         3.6.0\n",
      "huggingface-hub              0.10.1\n",
      "idna                         3.3\n",
      "imbalanced-learn             0.9.0\n",
      "imblearn                     0.0\n",
      "importlib-metadata           4.11.3\n",
      "importlib-resources          5.7.1\n",
      "ipykernel                    6.13.0\n",
      "ipython                      7.33.0\n",
      "ipython-genutils             0.2.0\n",
      "itsdangerous                 2.1.2\n",
      "jedi                         0.18.1\n",
      "Jinja2                       3.1.2\n",
      "joblib                       1.1.0\n",
      "jsonschema                   4.4.0\n",
      "jupyter-client               7.3.0\n",
      "jupyter-core                 4.10.0\n",
      "jupyterlab-pygments          0.2.2\n",
      "keras                        2.9.0\n",
      "Keras-Applications           1.0.8\n",
      "Keras-Preprocessing          1.1.2\n",
      "kiwisolver                   1.4.3\n",
      "langcodes                    3.3.0\n",
      "libclang                     14.0.1\n",
      "Markdown                     3.3.6\n",
      "MarkupSafe                   2.1.1\n",
      "matplotlib                   3.5.2\n",
      "matplotlib-inline            0.1.3\n",
      "mistune                      0.8.4\n",
      "murmurhash                   1.0.8\n",
      "nbclient                     0.6.0\n",
      "nbconvert                    6.5.0\n",
      "nbformat                     5.3.0\n",
      "nest-asyncio                 1.5.5\n",
      "nltk                         3.7\n",
      "notebook                     6.4.11\n",
      "numpy                        1.21.6\n",
      "oauthlib                     3.2.0\n",
      "opencv-python                4.5.5.64\n",
      "opt-einsum                   3.3.0\n",
      "packaging                    21.3\n",
      "pandas                       1.1.5\n",
      "pandocfilters                1.5.0\n",
      "parso                        0.8.3\n",
      "pathtools                    0.1.2\n",
      "pathy                        0.6.2\n",
      "pickleshare                  0.7.5\n",
      "Pillow                       9.1.1\n",
      "pip                          22.2.2\n",
      "pipreqs                      0.4.11\n",
      "platformdirs                 2.5.2\n",
      "preshed                      3.0.7\n",
      "prometheus-client            0.14.1\n",
      "promise                      2.3\n",
      "prompt-toolkit               3.0.29\n",
      "protobuf                     3.19.4\n",
      "psutil                       5.9.0\n",
      "pyasn1                       0.4.8\n",
      "pyasn1-modules               0.2.8\n",
      "pycparser                    2.21\n",
      "pydantic                     1.9.2\n",
      "Pygments                     2.12.0\n",
      "pyparsing                    3.0.8\n",
      "pyrsistent                   0.18.1\n",
      "python-dateutil              2.8.2\n",
      "pytorch-lightning            0.7.5\n",
      "pytz                         2022.1\n",
      "pywin32                      303\n",
      "pywinpty                     2.0.5\n",
      "PyYAML                       6.0\n",
      "pyzmq                        22.3.0\n",
      "regex                        2022.8.17\n",
      "requests                     2.27.1\n",
      "requests-oauthlib            1.3.1\n",
      "rsa                          4.8\n",
      "sacremoses                   0.0.53\n",
      "scikit-learn                 1.0.2\n",
      "scipy                        1.7.3\n",
      "seaborn                      0.11.2\n",
      "Send2Trash                   1.8.0\n",
      "sentencepiece                0.1.97\n",
      "sentry-sdk                   1.9.10\n",
      "setproctitle                 1.3.2\n",
      "setuptools                   65.4.0\n",
      "shortuuid                    1.0.9\n",
      "six                          1.16.0\n",
      "sklearn                      0.0\n",
      "smart-open                   5.2.1\n",
      "smmap                        5.0.0\n",
      "soupsieve                    2.3.2.post1\n",
      "spacy                        3.4.1\n",
      "spacy-legacy                 3.0.10\n",
      "spacy-loggers                1.0.3\n",
      "srsly                        2.4.4\n",
      "stop-words                   2018.7.23\n",
      "tabulate                     0.8.10\n",
      "tensorboard                  2.9.1\n",
      "tensorboard-data-server      0.6.1\n",
      "tensorboard-plugin-wit       1.8.1\n",
      "tensorflow                   2.9.1\n",
      "tensorflow-estimator         2.9.0\n",
      "tensorflow-io-gcs-filesystem 0.26.0\n",
      "termcolor                    1.1.0\n",
      "terminado                    0.13.3\n",
      "thinc                        8.1.1\n",
      "threadpoolctl                3.1.0\n",
      "tinycss2                     1.1.1\n",
      "tokenizers                   0.7.0\n",
      "torch                        1.12.1\n",
      "tornado                      6.1\n",
      "tqdm                         4.64.0\n",
      "traitlets                    5.1.1\n",
      "transformers                 2.9.0\n",
      "typer                        0.4.2\n",
      "typing_extensions            4.1.1\n",
      "urllib3                      1.26.12\n",
      "virtualenv                   20.16.5\n",
      "wandb                        0.13.4\n",
      "wasabi                       0.10.1\n",
      "wcwidth                      0.2.5\n",
      "webencodings                 0.5.1\n",
      "Werkzeug                     2.1.2\n",
      "wheel                        0.37.1\n",
      "wrapt                        1.14.0\n",
      "yarg                         0.1.9\n",
      "zipp                         3.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f3932e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
